{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Notebook\n",
    "This notebook loads the AutoGluon model, the normal PyTorch model, and the QAT PyTorch model. It evaluates their accuracies on the test dataset, compares their architectures and weights, and provides visual comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from torchviz import make_dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './datasets/CICIDS2017/balanced_binary'\n",
    "train_data = pd.read_csv(f\"{data_dir}/train.csv\")\n",
    "test_data = pd.read_csv(f\"{data_dir}/test.csv\")\n",
    "val_data = pd.read_csv(f\"{data_dir}/validation.csv\")\n",
    "\n",
    "# Drop the ID column\n",
    "train_data = train_data.drop(columns=['ID'])\n",
    "test_data = test_data.drop(columns=['ID'])\n",
    "val_data = val_data.drop(columns=['ID'])\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['Label'] = label_encoder.fit_transform(train_data['Label'])\n",
    "test_data['Label'] = label_encoder.transform(test_data['Label'])\n",
    "val_data['Label'] = label_encoder.transform(val_data['Label'])\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_data, test_data, val_data, batch_size=256):\n",
    "    X_train, y_train = train_data.drop(columns=['Label']).values, train_data['Label'].values\n",
    "    X_test, y_test = test_data.drop(columns=['Label']).values, test_data['Label'].values\n",
    "    X_val, y_val = val_data.drop(columns=['Label']).values, val_data['Label'].values\n",
    "\n",
    "    train_tensor = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    test_tensor = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "    val_tensor = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    \n",
    "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_tensor, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_tensor, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "train_loader, test_loader, val_loader = create_dataloaders(train_data, test_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the AutoGluon predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_path = './datasets/CICIDS2017/balanced_binary/automl_search'\n",
    "predictor = TabularPredictor.load(predictor_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model architecture class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoReplicatedNN(nn.Module):\n",
    "    def __init__(self, architecture, input_feature_size):\n",
    "        super(AutoReplicatedNN, self).__init__()\n",
    "        layers = []\n",
    "        current_input_size = input_feature_size\n",
    "        for layer_type, layer_obj in architecture:\n",
    "            if layer_type == nn.BatchNorm1d:\n",
    "                layers.append(nn.Identity())\n",
    "            elif layer_type == nn.Linear:\n",
    "                layers.append(nn.Linear(current_input_size, layer_obj.out_features))\n",
    "                current_input_size = layer_obj.out_features\n",
    "            elif layer_type == nn.ReLU:\n",
    "                layers.append(nn.ReLU())\n",
    "            elif layer_type == nn.Dropout:\n",
    "                layers.append(nn.Dropout(p=layer_obj.p))\n",
    "            elif layer_type != nn.Softmax:\n",
    "                raise ValueError(f\"Unhandled layer type: {layer_type}\")\n",
    "        self.main_block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the QAT wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QATWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(QATWrapper, self).__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.model = model\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_class, model_path, architecture=None, input_feature_size=None, device='cpu'):\n",
    "    if architecture and input_feature_size:\n",
    "        model = model_class(architecture, input_feature_size)\n",
    "    else:\n",
    "        model = model_class()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_model_path = './datasets/CICIDS2017/balanced_binary/compressed_models/normal_pytorch_model_exp3.pth'\n",
    "qat_model_path = './datasets/CICIDS2017/balanced_binary/compressed_models/qat_pytorch_model_exp3.pth'\n",
    "\n",
    "def get_model_architecture(predictor, input_feature_size):\n",
    "    model = predictor._trainer.load_best_model()\n",
    "    architecture = []\n",
    "    for name, module in model.named_children():\n",
    "        architecture.append((type(module), module))\n",
    "    return architecture, model, input_feature_size\n",
    "\n",
    "architecture, best_model, input_feature_size = get_model_architecture(predictor, len(train_data.columns) - 1)\n",
    "\n",
    "normal_model = load_model(AutoReplicatedNN, normal_model_path, architecture, input_feature_size)\n",
    "qat_model = load_model(QATWrapper, qat_model_path, architecture, input_feature_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "normal_accuracy = evaluate_model(normal_model, test_loader, device)\n",
    "qat_accuracy = evaluate_model(qat_model, test_loader, device)\n",
    "original_ag_model_accuracy = predictor.evaluate(test_data)['accuracy']\n",
    "\n",
    "print(f'Accuracy of the AutoGluon model on the test dataset: {original_ag_model_accuracy * 100:.2f}%')\n",
    "print(f'Accuracy of the normal PyTorch model on the test dataset: {normal_accuracy * 100:.2f}%')\n",
    "print(f'Accuracy of the QAT PyTorch model on the test dataset: {qat_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_architectures(model1, model2):\n",
    "    print(\"Model 1 Architecture:\")\n",
    "    print(model1)\n",
    "    print(\"\\nModel 2 Architecture:\")\n",
    "    print(model2)\n",
    "\n",
    "compare_architectures(normal_model, qat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_weights(model1, model2):\n",
    "    for (name1, param1), (name2, param2) in zip(model1.named_parameters(), model2.named_parameters()):\n",
    "        if torch.equal(param1, param2):\n",
    "            print(f\"Weights of layer {name1} and {name2} are equal.\")\n",
    "        else:\n",
    "            print(f\"Weights of layer {name1} and {name2} are different.\")\n",
    "\n",
    "compare_weights(normal_model, qat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(model, title):\n",
    "    weights = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            weights.append(param.data.cpu().numpy().flatten())\n",
    "    weights = np.concatenate(weights)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(weights, bins=100)\n",
    "    plt.title(f'Weight Distribution: {title}')\n",
    "    plt.xlabel('Weight')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_weights(normal_model, \"Normal PyTorch Model\")\n",
    "plot_weights(qat_model, \"QAT PyTorch Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_architecture(model, title):\n",
    "    x = torch.randn(1, len(train_data.columns) - 1).to(device)\n",
    "    y = model(x)\n",
    "    make_dot(y.mean(), params=dict(model.named_parameters())).render(title, format=\"png\")\n",
    "\n",
    "plot_model_architecture(normal_model, \"Normal PyTorch Model Architecture\")\n",
    "plot_model_architecture(qat_model, \"QAT PyTorch Model Architecture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
